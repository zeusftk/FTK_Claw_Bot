# FreeLLM Client

Python LLM å®¢æˆ·ç«¯ï¼Œé€šè¿‡æœ¬åœ° FreeLLM server è°ƒç”¨ LLMï¼Œæ— éœ€é…ç½® API keyã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **é›¶é…ç½®å¯åŠ¨** - è‡ªåŠ¨å¯åŠ¨æœ¬åœ° LLM æœåŠ¡
- ğŸ”Œ **OpenAI å…¼å®¹ API** - æä¾›æ ‡å‡†çš„ OpenAI API æ¥å£
- ğŸ–¥ï¸ **å›¾å½¢ç•Œé¢** - PyQt6 ç°ä»£åŒ–æ·±è‰²ä¸»é¢˜ç•Œé¢
- ğŸ“¦ **WSL æ”¯æŒ** - åœ¨ WSL åˆ†å‘ä¸­ç®¡ç†å¤šä¸ª LLM æœåŠ¡
- ğŸ”„ **è‡ªåŠ¨ç«¯å£åˆ†é…** - è‡ªåŠ¨ä¸ºæ¯ä¸ªæœåŠ¡åˆ†é…ç‹¬ç«‹ç«¯å£

## å®‰è£…

```bash
pip install -e .
```

## ä½¿ç”¨

### å‘½ä»¤è¡Œ

```bash
# å¯åŠ¨å›¾å½¢ç•Œé¢
freellm-client

# æˆ–
python -m freellm
```

### Python API

```python
from freellm import FreeLLMClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = FreeLLMClient()

# å‘é€æ¶ˆæ¯
result = client.chat("ä½ å¥½ï¼")
print(result.message.text)

# ä½¿ç”¨æŒ‡å®šæ¨¡å‹
result = client.chat("Hello!", model="freellm/glm-5-free")

# æµå¼è¾“å‡º
for chunk in client.chat_stream("å†™ä¸€é¦–è¯—"):
    print(chunk.get("text", ""), end="")

# ä½¿ç”¨ä¸Šä¸‹æ–‡
with FreeLLMClient() as client:
    result = client.chat("ä½ å¥½")
    print(result.message.text)
```

## æœåŠ¡ç®¡ç†

### ç«¯å£åˆ†é…

| æœåŠ¡ | ç«¯å£èŒƒå›´ |
|------|----------|
| LLM æœåŠ¡ | 20100-20199 |
| Router æœåŠ¡ | 20200-20299 |

### å›¾å½¢ç•Œé¢

1. é€‰æ‹© WSL åˆ†å‘
2. é…ç½®ç«¯å£ï¼ˆå¯é€‰ï¼‰
3. ç‚¹å‡»"å¯åŠ¨"æŒ‰é’®
4. é€šè¿‡ `http://127.0.0.1:20200` è®¿é—® OpenAI å…¼å®¹ API

## API ç«¯ç‚¹

Router æœåŠ¡æä¾›ä»¥ä¸‹ç«¯ç‚¹ï¼š

- `GET /health` - å¥åº·æ£€æŸ¥
- `POST /v1/chat/completions` - OpenAI å…¼å®¹èŠå¤©æ¥å£
- `GET /v1/models` - åˆ—å‡ºå¯ç”¨æ¨¡å‹

## å¼€å‘

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# è¿è¡Œæµ‹è¯•
pytest

# ä»£ç æ ¼å¼åŒ–
black .
ruff check .
```

## è®¸å¯è¯

MIT License
